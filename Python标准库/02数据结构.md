---
title: 数据结构
date: 2016-08-12 09:56
category:
tags:
---

## Queue -- 线程安全
### 支持的类型和基本操作
```python
import Queue

q1 = Queue.Queue()          # 先入先出队列
q2 = Queue.LifoQueue()      # 先入后出队列
q3 = Queue.PriorityQueue()  # 优先队列

q.put(item)
q.pop(item)
q.empty()
```

### 优先队列与多线程
```python
import Queue
import threading

class Job(object):
    def __init__(self, priority, description):
        self.priority = priority
        self.description = description

        print "start Job: ", self.description

    def __cmp__(self, other):
        return cmp(self.priority, other.priority)

q = Queue.PriorityQueue()
q.put(Job(2, "normal job"))             # 越小权重越高
q.put(Job(10, "the last job"))
q.put(Job(0, "urgent job"))

def schedule(qwork):
    while True:
        next = qwork.get()      # cmp each job and return the max one
        print next.description, " is processing..."
        q.task_done()           # 向任务已经完成的队列发送一个信号?

workers = [
            threading.Thread(target=schedule, args=(q,)),
            threading.Thread(target=schedule, args=(q, ))
            ]

for w in workers:
    w.setDaemon(True)
    w.start()

q.join()
```

### 一个多线程例子
```python
import urllib
import urlparse
from Queue import Queue
from threading import Thread

import feedparser

num_fetch_threads = 5
enclosure_queue = Queue()

feed_urls = ['www.reuters.com/tools/rss']

# 下载器
def downloadEnclosures(i, q):
    while True:
        print i, 'Looking for the next enclosure'
        url = q.get()
        parsed_url = urlparse.urlparse(url)
        print i, 'Downloading: ', parsed_url.path
        response = urllib.urlopen(url)
        data = response.read()

        outfile_name = url.rpartition('/')[-1]      # 获取文件名
        with open(outfile_name, 'wb') as outfile:   # 存至本地
            outfile.write(data)
        print "saved"
        q.task_done()

# 启动执行下载的线程

for i in range(num_fetch_threads):
    worker = Thread(target=downloadEnclosures, args=(i, enclosure_queue,))
    worker.setDaemon(True)
    worker.start()


# 下载feed并添加到队列
for url in feed_urls:
    response = feedparser.parse(url, agent='fetch_prodcasts.py')

for entry in response['entries'][-5:]:
    for enclosure in entry.get('enclosures', []):
        parsed_url = urlparse.urlparse(enclosure['url'])
        print 'Queuing: ', parsed_url.path
        enclosure_queue.pyt(enclosure['url'])

print '### main thread waiting...'
enclosure_queue.join()
print '### Done!'
```
